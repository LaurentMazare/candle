[package]
name = "candle-flash-attn"
version = "0.1.0"
edition = "2021"

description = "Flash attention layer for the candle ML framework."
repository = "https://github.com/LaurentMazare/candle"
keywords = ["blas", "tensor", "machine-learning"]
categories = ["science"]
license = "MIT/Apache-2.0"
readme = "README.md"

[dependencies]
candle = { path = "../candle-core", features = ["cuda"] }
half = { workspace = true }

[build-dependencies]
anyhow = { workspace = true }
